# HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild

## Data

In our preliminary experiments, we observed that some challenging queries do not fall into our predefined 5 categories and designed a category called 'other types' in our original dataset construction.  When evaluating with the final benchmark, we decided to remove this category merely to make the whole evaluation process more clear.

For the sake of completeness and to cover a broader range of hallucination types, we provide the queries of these 'other types' in the HaluEval-Wild dataset release.

## License

HaluEval-Wild uses [Apache-2.0 License](./LICENSE).
